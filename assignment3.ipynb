{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3: Vision Transformer and Contrastive Representation Learning\n",
    "\n",
    "In this assignment, first, you are to implement [Vision Transformer](https://arxiv.org/pdf/2010.11929v2.pdf) (ViT) for image recognition. Then, you are utilize the implemented ViT to train a model using [Contrastive Representation Learning](https://arxiv.org/pdf/1503.03832.pdf), i.e. Triplet Loss. \n",
    "\n",
    "### Submission Guidelines\n",
    "The assignment codebase is split into two files. `vit_model.py` and `assignment3.ipynb`. In `vit_model.py`, incomplete codes for ViT and embedding model are provided, while in `assignment3.ipynb`, incomplete codes to iterate [FashionMNIST dataset](https://github.com/zalandoresearch/fashion-mnist) and initiate training and testing are provided in blocks. You are to complete the missing codeblock (Marked with comments and #TODO signs) and train the embedding representation model. \n",
    "\n",
    "In addition to PyTorch, [pytorch-metric-learning](https://github.com/KevinMusgrave/pytorch-metric-learning) is needed to help with the implementation of contrastive representation learning. You are strongly encouraged to go through the [README of the repo](https://github.com/KevinMusgrave/pytorch-metric-learning/blob/master/README.md), which contains the basic usage of the package. You are also strongly encouraged to study the example notebooks provided within the repo, at your own pace.\n",
    "\n",
    "To submit this assignment for grading, you must submit a compressed zipfile as `Assignment3_{YourCCID}.zip`, with three files inside: `vit_model.py`, `assignment3.ipynb`, and your trained embedding model as `vit_embeds.pt`. \n",
    "\n",
    "### Collaboration Policy\n",
    "This must be your own work. Do not share or look at the code of other students (whether they are inside or outside the class). Do not copy the code from the Internet, other than referring [PyTorch's official tutorials](https://pytorch.org/tutorials/), or the examples provided within [pytorch-metric-learning repo](https://github.com/KevinMusgrave/pytorch-metric-learning/tree/master/examples). You can talk to others in the class about solution ideas (but detailed enough that you are verbally sharing, hearing or seeing the code). You must cite whom you talked with in the comments of your programs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Make sure the external packages are installed in Colab\n",
    "!pip install pytorch-metric-learning\n",
    "!pip install faiss-gpu\n",
    "!pip install matplotlib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports for this assignment\n",
    "# NOTE: DO NOT MODIFY UNLESS NECESSARY\n",
    "import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, RandomHorizontalFlip, RandomResizedCrop, RandomRotation\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_metric_learning import miners, losses, distances\n",
    "from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab Patch\n",
    "use_colab = True\n",
    "if use_colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    import sys\n",
    "    # ----------------------------------------\n",
    "    dir = \"/content/drive/MyDrive/Colab Notebooks\"    # TODO: MODIFY THIS TO INDICATE THE PARENT FOLDER OF YOUR vit_model.py file\n",
    "    # ----------------------------------------\n",
    "    sys.path.append(dir)\n",
    "\n",
    "from vit_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "# NOTE: Feel free to add & modify hyperparameters if needed\n",
    "num_epochs = 50\n",
    "batch_size = 512\n",
    "weight_decay = 1e-4\n",
    "lr = 3e-4       # TODO: Use appropriate LR with LR scheduler\n",
    "\n",
    "# ViT specifics\n",
    "image_size = 28\n",
    "in_channels = 1\n",
    "patch_size = 4\n",
    "hidden_size = 64\n",
    "layers = 6\n",
    "heads = 8\n",
    "embed_size = 64\n",
    "\n",
    "# Contrastive Learning specifics\n",
    "margin = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Data Augmentation\n",
    "\n",
    "For the dataset, we will use the FashionMNIST dataset. For implmentation, you are explore the usage of different [data augmentation](https://pytorch.org/vision/master/auto_examples/transforms/plot_transforms_illustrations.html) techniques. \n",
    "\n",
    "**(1 out of 8)** You are to use at least 2 different types of data augmentation techniques (e.g. RandomHorizontalFlip) for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FashionMNIST dataset\n",
    "classes = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "num_classes = len(classes)\n",
    "\n",
    "tfm_train = Compose([\n",
    "    # TODO: Add your data augmentation HERE\n",
    "    # ####################\n",
    "    RandomHorizontalFlip(),\n",
    "    RandomResizedCrop((32,32),scale=(0.8,1.0),ratio=(0.9,1.1)),\n",
    "    RandomRotation(degrees=(-13, 13)),\n",
    "    # ####################\n",
    "    ToTensor(),\n",
    "    Normalize((0.5, ), (0.5, )),\n",
    "    ])\n",
    "\n",
    "tfm_test = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize((0.5, ), (0.5, )),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Inspect your data augmentation\n",
    "def inverse_transform(\n",
    "    img_tensor: torch.Tensor,\n",
    "    ) -> np.ndarray:\n",
    "    \"\"\"Given a preprocessed image tensor, revert the normalization process and \n",
    "    convert the tensor back to a numpy image.\n",
    "    \"\"\"\n",
    "    inv_normalize = Normalize(mean=(-0.5/0.5, ), std=(1/0.5, ))\n",
    "    img_tensor = inv_normalize(img_tensor).permute(1, 2, 0)\n",
    "    img = np.uint8(255 * img_tensor.numpy())\n",
    "    return img\n",
    "\n",
    "# Get some random training images\n",
    "n_imgs = 5\n",
    "dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=tfm_train)\n",
    "indices = np.random.randint(0, len(dataset), size=(n_imgs, ))\n",
    "\n",
    "# Visualize with matplotlib\n",
    "for i, idx in enumerate(indices):\n",
    "    img_tensor, label = dataset[idx]\n",
    "    img = inverse_transform(img_tensor)\n",
    "    plt.subplot(1, n_imgs, i + 1)\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "    plt.title(classes[label])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Vision Transformer\n",
    "**(2 out of 8)** For Part 1, you are to finish the implmentation of ViT, following the discussed lab instructions. Incomplete codes are provided in file `vit_model.py`. \n",
    "\n",
    "A `check_vit.py` test script is provided to inspect the correctness of your implementations. Make sure not to modify any code in the provided test script.\n",
    "\n",
    "Additionally, the code to train a classifier is provided below. If your ViT implementation is correct, a trained model should reach 88% accuracy and above with no issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Classifier\n",
    "# -----\n",
    "\n",
    "def train_classification_model():\n",
    "    # Base ViT\n",
    "    vit_model = ViT(\n",
    "        image_size=image_size, \n",
    "        patch_size=patch_size, \n",
    "        num_channels=in_channels, \n",
    "        hidden_size=hidden_size, \n",
    "        layers=layers, \n",
    "        heads=heads)\n",
    "\n",
    "    # Classifier\n",
    "    model_classifier = ClassificationHead(hidden_size=vit_model.hidden_size, num_classes=num_classes)\n",
    "    if torch.cuda.is_available():\n",
    "        vit_model = vit_model.cuda()\n",
    "        model_classifier = model_classifier.cuda()\n",
    "\n",
    "    # Use cross entropy\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Specify optimizer\n",
    "    parameters = list(vit_model.parameters()) + list(model_classifier.parameters())\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        parameters, \n",
    "        lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # Evaluate at the end of each epoch\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (x, labels) in enumerate(train_loader):\n",
    "            vit_model.train()\n",
    "            model_classifier.train()\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                x = x.cuda()\n",
    "                labels = labels.cuda()\n",
    "            \n",
    "            # Forward pass\n",
    "            feats = vit_model(x)\n",
    "            outputs = model_classifier(feats)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # NOTE: Show train loss at the end of epoch\n",
    "            # Feel free to modify this to log more steps\n",
    "            if (i+1) % 3 == 0:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                    .format(epoch+1, num_epochs, i+1, len(train_loader), loss.item()))\n",
    "\n",
    "        # Evaluate at the end\n",
    "        test_acc = test_classification_model(vit_model, model_classifier)\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            state_dict = {\n",
    "                \"classifier\": model_classifier.state_dict(),\n",
    "                \"vit\": vit_model.state_dict(),\n",
    "                \"acc\": best_acc,\n",
    "            }\n",
    "            torch.save(state_dict, \"vit_classifier.pt\")\n",
    "            print(\"Best test acc:\", best_acc)\n",
    "        print()\n",
    "    print(\"Best test acc:\", best_acc)\n",
    "\n",
    "def test_classification_model(\n",
    "    vit_model: nn.Module,\n",
    "    model_classifier: nn.Module,\n",
    "    ):\n",
    "    # Test the model\n",
    "    vit_model.eval()\n",
    "    model_classifier.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            if torch.cuda.is_available():\n",
    "                images = images.cuda()\n",
    "                labels = labels.cuda()\n",
    "            feats = vit_model(images)\n",
    "            outputs = model_classifier(feats)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        print('Test Accuracy: {} %'.format(100 * correct / total))\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Uncomment this to test your ViT implementation\n",
    "\n",
    "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=tfm_train)\n",
    "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=tfm_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "train_classification_model()\n",
    "\n",
    "del train_dataset\n",
    "del test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Contrastive Representation Learning\n",
    "In this part, you are to use [PyTorch-Metric-Learning](https://github.com/KevinMusgrave/pytorch-metric-learning) to implement triplet loss and learn an embedding representation model with ViT. Quick tutorials and notebooks are available in the repo. \n",
    "\n",
    "However, only a subset of data from the FashionMNIST training set (around 1/12) will be used to train the model. The code to subsample the full train dataset is provided in `sample_balanced_subset`. You must not modfiy the code in that block. \n",
    "\n",
    "To pass this part, you are to reach **60%** and above in Precision@1 metric scoring on the full FashionMNIST test dataset with your trained embedding model. The final model must be submitted along with your notebook with logging outputs. The test function is provided in `evaluate_at_end_epoch`. \n",
    "\n",
    "Furthermore, you are required to employ one of the learning rate schedulers, like [MultiStepLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.MultiStepLR.html#torch.optim.lr_scheduler.MultiStepLR), [Consine Annealing LR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR). Or you can implement your own learning rate adjusting strategy, as a function to training iteration or epoch, using [LambdaLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LambdaLR.html#torch.optim.lr_scheduler.LambdaLR). The marking is distributed as follows:\n",
    "\n",
    "- (2 out of 8) Complete `train_embedding_model`. Add your selected learning rate scheduler, and train your embedding model\n",
    "- (0.5 out of 8) Complete `get_embeddings` to correctly perform inference with your embedding model.\n",
    "- (1 out of 8) Attach the notebook with full training logs (do not clear your notebook outputs when finished running), and your trained model file with **60%** and above in Precision@1 metric scoring, as `vit_embeds.pt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a subset of training dataset\n",
    "# NOTE: sample an even number of samples per class\n",
    "# To migrate the problem of [class imbalancing]\n",
    "# (https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/imbalanced-data).\n",
    "def sample_balanced_subset(\n",
    "    train_dataset,\n",
    "    n_samples_per_class: int = 50,\n",
    "    n_classes: int = 10\n",
    "    ):\n",
    "    maps = {i: [] for i in range(n_classes)}\n",
    "    for idx, (_, cls_idx) in enumerate(train_dataset):\n",
    "        if len(maps[cls_idx]) < n_samples_per_class:\n",
    "            maps[cls_idx].append(idx)\n",
    "\n",
    "    indices = []\n",
    "    for _, ind in maps.items():\n",
    "        indices += ind\n",
    "\n",
    "    train_subset = torch.utils.data.Subset(train_dataset, indices)\n",
    "    print(\"Num. samples in subset:\", len(train_subset))\n",
    "    return train_subset\n",
    "\n",
    "# Subsample a small subset from the full train dataset\n",
    "full_train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=tfm_train)\n",
    "train_dataset = sample_balanced_subset(full_train_dataset, n_samples_per_class=500)      # DO NOT MODIFY THIS CODE\n",
    "\n",
    "# Full test set will still be used for testing\n",
    "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=tfm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an embedding model\n",
    "# -----\n",
    "\n",
    "def train_embedding_model(\n",
    "    train_dataset,\n",
    "    test_dataset,\n",
    "    ):\n",
    "    #########################\n",
    "    # Finish Your Code HERE\n",
    "    # #########################\n",
    "\n",
    "    # Dataloader\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    # Evaluate model per epoch, and keep track of the model performance at that time\n",
    "    best_prec = 0.0\n",
    "\n",
    "    # Triplet Loss with Semi-hard triples & l2 distance\n",
    "    # -------------------\n",
    "    # TODO: Specify distance, objective function (triplet loss), and your miner to sample triplet pairs\n",
    "\n",
    "    distance = None\n",
    "    criterion = None\n",
    "    miner = None        # Make sure to use \"semihard\" triplets\n",
    "\n",
    "    # -------------------\n",
    "\n",
    "    # Precision to test embedding quality\n",
    "    accuracy_calculator = AccuracyCalculator(include=(\"precision_at_1\",), k=1)\n",
    "\n",
    "    # Models to train\n",
    "    # -------------------\n",
    "    # TODO: Specify your model(s) correctly here\n",
    "    # Don't forget to send them to cuda/gpu\n",
    "    vit_model = None\n",
    "    model_embedding = None\n",
    "\n",
    "    # -------------------\n",
    "\n",
    "    # Specify optimizer\n",
    "    # -------------------\n",
    "    parameters = None       # TODO: correctly specify the trainable parameters here\n",
    "\n",
    "    optimizer = None        # TODO: Specify your optimizer\n",
    "\n",
    "    scheduler = None        # TODO: Specify your LR scheduler\n",
    "\n",
    "    # -------------------\n",
    "\n",
    "    # Train loop\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (x, labels) in enumerate(train_loader):\n",
    "            vit_model.train()\n",
    "            model_embedding.train()\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                x = x.cuda()\n",
    "                labels = labels.cuda()\n",
    "            \n",
    "            # Forward pass\n",
    "            # -------------------\n",
    "            # TODO: Implement forward and backward pass\n",
    "            # TODO: Correctly update your LR with your selected LR scheduler\n",
    "\n",
    "\n",
    "            # -------------------\n",
    "\n",
    "            # NOTE: Show train loss at the end of epoch\n",
    "            # Feel free to modify this to log more steps\n",
    "            if (i+1) % len(train_loader) == 0:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                    .format(epoch+1, num_epochs, i+1, len(train_loader), loss.item()))\n",
    "\n",
    "        # Evaluate at the end\n",
    "        best_prec = evaluate_at_end_epoch(vit_model, model_embedding, accuracy_calculator, best_prec, train_dataset, test_dataset)\n",
    "\n",
    "    # #########################\n",
    "\n",
    "def get_embeddings(\n",
    "    x: torch.Tensor, \n",
    "    vit_model: nn.Module, \n",
    "    model_embedding: nn.Module,\n",
    "    ) -> torch.Tensor:\n",
    "    \"\"\"Calculate embeddings for a batch of images.\n",
    "    \"\"\"\n",
    "    #########################\n",
    "    # Finish Your Code HERE\n",
    "    # #########################\n",
    "    # TODO: Correctly calculate x_embeds for a batch of \n",
    "    # images x with shape (batch_size, 3, h, w)\n",
    "\n",
    "    \n",
    "\n",
    "    #########################\n",
    "\n",
    "    x_embeds = x_embeds.cpu()   # Cast to CPU\n",
    "    x_embeds = torch.nn.functional.normalize(x_embeds, p=2, dim=1)      # Extra Step: Normalize the embeddings\n",
    "    return x_embeds\n",
    "\n",
    "def get_embeddings_over_dataset(\n",
    "    dataset,\n",
    "    vit_model: nn.Module,\n",
    "    model_embedding: nn.Module,\n",
    "    ):\n",
    "    \"\"\"Loop through a full dataset and return all embeddings.\n",
    "    \"\"\"\n",
    "    # Create a loader on the go\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    X_embeds, Y = [], []\n",
    "    for i, (x, y) in enumerate(tqdm.tqdm(loader)):\n",
    "        x_embeds = get_embeddings(x, vit_model, model_embedding)\n",
    "        X_embeds.append(x_embeds)\n",
    "        Y.append(y)\n",
    "\n",
    "    X_embeds = torch.cat(X_embeds, dim=0)\n",
    "    Y = torch.cat(Y, dim=0)\n",
    "    return X_embeds, Y\n",
    "\n",
    "def test_embedding_model(\n",
    "    vit_model: nn.Module,\n",
    "    model_embedding: nn.Module,\n",
    "    accuracy_calculator,\n",
    "    train_dataset,\n",
    "    test_dataset,\n",
    "    ):\n",
    "    # Test the model\n",
    "    model_embedding.eval()\n",
    "\n",
    "    X_embeds, Y = get_embeddings_over_dataset(train_dataset, vit_model, model_embedding)\n",
    "    X_embeds_test, Y_test = get_embeddings_over_dataset(test_dataset, vit_model, model_embedding)\n",
    "    accuracies = accuracy_calculator.get_accuracy(\n",
    "        X_embeds_test, Y_test, X_embeds, Y, False\n",
    "    )\n",
    "    return accuracies[\"precision_at_1\"]\n",
    "\n",
    "def evaluate_at_end_epoch(\n",
    "    vit_model: nn.Module,\n",
    "    model_embedding: nn.Module,\n",
    "    accuracy_calculator,\n",
    "    best_prec: float,\n",
    "    train_dataset,\n",
    "    test_dataset,\n",
    "    ):\n",
    "    # Evaluate at the end\n",
    "    prec = test_embedding_model(vit_model, model_embedding, accuracy_calculator, train_dataset, test_dataset)\n",
    "    if prec > best_prec:\n",
    "        best_prec = prec\n",
    "        state_dict = {\n",
    "            \"embedding_head\": model_embedding.state_dict(),\n",
    "            \"vit\": vit_model.state_dict(),\n",
    "            \"precision@1\": prec,\n",
    "        }\n",
    "        torch.save(state_dict, \"vit_embeds.pt\")\n",
    "        print(\"Best Precision@1:\", best_prec)\n",
    "    print()\n",
    "    return best_prec\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Uncomment this to train your embedding model\n",
    "# torch.cuda.empty_cache()    # Clean-up memory 1st\n",
    "# train_embedding_model(train_dataset, test_dataset)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Application\n",
    "In the below examples, we perform inference with the trained embedding model using K-nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()    # Clean-up memory 1st\n",
    "\n",
    "# Load your trained model\n",
    "# Base ViT\n",
    "vit_model = ViT(\n",
    "    image_size=image_size, \n",
    "    patch_size=patch_size, \n",
    "    num_channels=in_channels, \n",
    "    hidden_size=hidden_size, \n",
    "    layers=layers, \n",
    "    heads=heads)\n",
    "\n",
    "# Embedding head\n",
    "model_embedding = LinearEmbeddingHead(hidden_size=vit_model.hidden_size, embed_size=embed_size)\n",
    "if torch.cuda.is_available():\n",
    "    vit_model = vit_model.cuda()\n",
    "    model_embedding = model_embedding.cuda()\n",
    "\n",
    "# Load saved checkpoint\n",
    "checkpoint = torch.load(\"vit_embeds.pt\")\n",
    "vit_model.load_state_dict(checkpoint[\"vit\"])\n",
    "model_embedding.load_state_dict(checkpoint[\"embedding_head\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To quickly infer K-nearest neighbors, we construct a small bank, where the (image, label) pairs in the bank are sampled from the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a bank, which is a small subset of trainset\n",
    "bank = sample_balanced_subset(train_dataset, n_samples_per_class=25)\n",
    "X_bank_embeds, Y_bank = get_embeddings_over_dataset(bank, vit_model=vit_model, model_embedding=model_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we perform the K-nearest neighbors, by comparing our sample against the embeddings in the bank. \n",
    "\n",
    "**(0.5 out of 8)** You are to finish the implementation of the function `retrieve_topk_nearest_neighbors_l2`, in order to acquire the demoing visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep KNN\n",
    "def retrieve_topk_nearest_neighbors_l2(\n",
    "    query: torch.Tensor,\n",
    "    ref: torch.Tensor,\n",
    "    k: int = 5,\n",
    "    ):\n",
    "    \"\"\"Calculate L2 distance between query and ref embeddings.\n",
    "    Return the top-k scores and the indices of the top-k scores.\n",
    "    \"\"\"\n",
    "    #########################\n",
    "    # Finish Your Code HERE\n",
    "    # #########################\n",
    "    # TODO: Implement L2 distance and topk ops\n",
    "\n",
    "\n",
    "    # #########################\n",
    "\n",
    "    return top_k_scores, indices\n",
    "\n",
    "# Grab some samples in test set\n",
    "dataiter = iter(test_loader)\n",
    "X, Y = next(dataiter)\n",
    "X_embeds = get_embeddings(X, vit_model=vit_model, model_embedding=model_embedding)\n",
    "\n",
    "# One random sample\n",
    "idx = np.random.randint(0, X.shape[0])\n",
    "img, label, embed = X[idx], Y[idx], X_embeds[idx]\n",
    "\n",
    "# Get top-k nearest samples\n",
    "values, indices = retrieve_topk_nearest_neighbors_l2(embed.unsqueeze(0), X_bank_embeds)\n",
    "\n",
    "# Visualize results\n",
    "_, axes = plt.subplots(1, 6, figsize=(18, 3))\n",
    "axes[0].imshow(inverse_transform(img), cmap=\"gray\")\n",
    "axes[0].set_title(classes[label])\n",
    "for i, ind in enumerate(indices):\n",
    "    img_bank, label_bank = bank[ind]\n",
    "    axes[i + 1].imshow(inverse_transform(img_bank), cmap=\"gray\")\n",
    "    axes[i + 1].set_title(\"{}\\nScore: {:.3f}\".format(classes[label_bank], values[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained embedding model can also be used to train a new classifier.\n",
    "\n",
    "The intuition behind **Transfer Learning**, and a referring tutorial can be seen [here](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html).\n",
    "\n",
    "**(1 out of 8)** Complete your code to train a classification head only, using your trained ViT embedding model. But you should not modify your trained ViT model weights in your `train_classification_model_head_only` function.\n",
    "\n",
    "A sample function to test with the classification head is provided, as `test_classification_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Classifier\n",
    "# -----\n",
    "\n",
    "def train_classification_model_head_only(\n",
    "    vit_model: nn.Module,\n",
    "    train_dataset, \n",
    "    test_dataset,\n",
    "    num_epochs: int = 10,\n",
    "    ):\n",
    "    #########################\n",
    "    # Finish Your Code HERE\n",
    "    # #########################\n",
    "\n",
    "    # Classifier\n",
    "    model_classifier = None\n",
    "    best_acc = 0.0\n",
    "    vit_model = None\n",
    "\n",
    "\n",
    "    # #########################\n",
    "    \n",
    "    # You should return the weights of your trained model, and the classification score (accuracy)\n",
    "    return {\"acc\": acc, \"vit\": vit_model.state_dict(), \"classifier\":model_classifier.state_dict()}\n",
    "\n",
    "def test_classification_model(\n",
    "    vit_model: nn.Module,\n",
    "    model_classifier: nn.Module,\n",
    "    ):\n",
    "    # Test the model\n",
    "    vit_model.eval()\n",
    "    model_classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            if torch.cuda.is_available():\n",
    "                images = images.cuda()\n",
    "                labels = labels.cuda()\n",
    "            feats = vit_model(images)\n",
    "            outputs = model_classifier(feats)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        # print('Test Accuracy: {} %'.format(100 * correct / total))\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a deep classifier in 5 epochs\n",
    "train_classification_model_head_only(vit_model, train_dataset, test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
